{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aomTSOlCo8P2"
      },
      "source": [
        "# Mount google drive to access the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0SzxixxicBSv"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "L6YbudkGpEVa"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A92YvEWjpD2G"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: arabert in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (1.0.1)\n",
            "Requirement already satisfied: farasapy in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from arabert) (0.0.14)\n",
            "Requirement already satisfied: PyArabic in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from arabert) (0.6.15)\n",
            "Requirement already satisfied: emoji==1.4.2 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from arabert) (1.4.2)\n",
            "Requirement already satisfied: tqdm in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from farasapy->arabert) (4.64.1)\n",
            "Requirement already satisfied: requests in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from farasapy->arabert) (2.28.1)\n",
            "Requirement already satisfied: six>=1.14.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from PyArabic->arabert) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests->farasapy->arabert) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests->farasapy->arabert) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests->farasapy->arabert) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests->farasapy->arabert) (1.26.14)\n",
            "Requirement already satisfied: colorama in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from tqdm->farasapy->arabert) (0.4.6)\n",
            "Requirement already satisfied: camel-tools in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (1.5.2)\n",
            "Requirement already satisfied: six in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from camel-tools) (1.16.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from camel-tools) (1.23.5)\n",
            "Requirement already satisfied: editdistance in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from camel-tools) (0.6.2)\n",
            "Requirement already satisfied: pandas in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from camel-tools) (1.5.3)\n",
            "Requirement already satisfied: scipy in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from camel-tools) (1.10.0)\n",
            "Requirement already satisfied: pyrsistent in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from camel-tools) (0.18.0)\n",
            "Requirement already satisfied: docopt in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from camel-tools) (0.6.2)\n",
            "Requirement already satisfied: dill in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from camel-tools) (0.3.6)\n",
            "Requirement already satisfied: transformers>=3.0.2 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from camel-tools) (4.24.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from camel-tools) (1.2.1)\n",
            "Requirement already satisfied: torch>=1.3 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from camel-tools) (1.12.1)\n",
            "Requirement already satisfied: requests in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from camel-tools) (2.28.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from camel-tools) (4.64.1)\n",
            "Requirement already satisfied: emoji in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from camel-tools) (1.4.2)\n",
            "Requirement already satisfied: cachetools in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from camel-tools) (5.3.1)\n",
            "Requirement already satisfied: future in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from camel-tools) (0.18.3)\n",
            "Requirement already satisfied: tabulate in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from camel-tools) (0.8.10)\n",
            "Requirement already satisfied: muddler in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from camel-tools) (0.1.3)\n",
            "Requirement already satisfied: typing_extensions in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from torch>=1.3->camel-tools) (4.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (0.15.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (0.11.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (22.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (2022.7.9)\n",
            "Requirement already satisfied: colorama in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from tqdm->camel-tools) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from pandas->camel-tools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from pandas->camel-tools) (2022.7)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests->camel-tools) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests->camel-tools) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests->camel-tools) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests->camel-tools) (1.26.14)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from scikit-learn->camel-tools) (2.2.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from scikit-learn->camel-tools) (1.1.1)\n",
            "Requirement already satisfied: fsspec in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers>=3.0.2->camel-tools) (2022.11.0)\n",
            "Requirement already satisfied: accelerate in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (0.20.3)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from accelerate) (22.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from accelerate) (1.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: typing_extensions in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from torch>=1.6.0->accelerate) (4.4.0)\n",
            "Requirement already satisfied: transformers[torch] in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (4.24.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from transformers[torch]) (2022.7.9)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from transformers[torch]) (22.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from transformers[torch]) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from transformers[torch]) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.15.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.11.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from transformers[torch]) (3.9.0)\n",
            "Requirement already satisfied: requests in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.28.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.7 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.12.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers[torch]) (4.4.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers[torch]) (2022.11.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (1.26.14)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2023.5.7)\n",
            "Requirement already satisfied: datasets in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (2.13.1)\n",
            "Requirement already satisfied: pandas in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from datasets) (22.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: xxhash in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from datasets) (12.0.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from datasets) (2.28.1)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from datasets) (0.15.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: evaluate in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (0.4.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from evaluate) (22.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from evaluate) (2022.11.0)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from evaluate) (4.64.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from evaluate) (2.13.1)\n",
            "Requirement already satisfied: dill in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: pandas in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from evaluate) (0.15.1)\n",
            "Requirement already satisfied: responses<0.19 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: xxhash in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from evaluate) (2.28.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (12.0.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2022.7)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: rouge_score in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (0.1.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: nltk in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from rouge_score) (3.7)\n",
            "Requirement already satisfied: six>=1.14.0 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: absl-py in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: click in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (8.0.4)\n",
            "Requirement already satisfied: joblib in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (1.1.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (2022.7.9)\n",
            "Requirement already satisfied: tqdm in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (4.64.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\mohamed\\anaconda3\\lib\\site-packages (from click->nltk->rouge_score) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install arabert\n",
        "!pip install camel-tools\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch]\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mTqureNNp5Zx"
      },
      "source": [
        "# Create a transformers dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dAVXEUMeiEBj"
      },
      "source": [
        "### 1. Import the required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "t6XIhTcRzcQE"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from csv import DictReader\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "from camel_tools.utils.normalize import normalize_unicode\n",
        "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
        "from camel_tools.utils.normalize import normalize_alef_ar\n",
        "from camel_tools.utils.normalize import normalize_teh_marbuta_ar"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0bz0K_8PiJCT"
      },
      "source": [
        "### 2. Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QQNpdu0OU57m"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2023-07-03 21:00:17,633 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
          ]
        }
      ],
      "source": [
        "stopwords = set(open('arabic_stopwords.txt').read().split())\n",
        "preprocessor = ArabertPreprocessor('aubmindlab/bert-base-arabertv2')\n",
        "\n",
        "def filter_stopwords(text):\n",
        "  return ' '.join(word for word in text.split() if word not in stopwords)\n",
        "\n",
        "def preprocess_text(text):\n",
        "  return normalize_teh_marbuta_ar(\n",
        "      normalize_alef_maksura_ar(\n",
        "          normalize_alef_ar(\n",
        "              normalize_unicode(\n",
        "                  preprocessor.preprocess(\n",
        "                      filter_stopwords(\n",
        "                          text\n",
        "                      )\n",
        "                  )\n",
        "              )\n",
        "          )\n",
        "      )\n",
        "  )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_EuYzINpiRjS"
      },
      "source": [
        "### 3. Read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t5TV2rdVp3Zz"
      },
      "outputs": [],
      "source": [
        "dataset = {'text': [], 'summary': []}\n",
        "\n",
        "with open(r'Datasets/ArabicMogalad_Ndeef.csv') as file:\n",
        "  reader = DictReader(file)\n",
        "\n",
        "  for row in reader:\n",
        "    dataset['text'].append(row.pop('Text').strip())\n",
        "    dataset['summary'].append(row.pop('Summary').strip())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNdNHpeziT6R"
      },
      "source": [
        "### 4. Convert to a transformers dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "52Yrt5h00F1F"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset.from_dict(dataset)\n",
        "dataset = dataset.train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZRgqY0zQSjEv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'summary'],\n",
              "        num_rows: 212380\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'summary'],\n",
              "        num_rows: 53096\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bOfzw3Bc0gKx"
      },
      "source": [
        "# Fine tuning AraGPT2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2LAU9UldLGgE"
      },
      "source": [
        "### 1. Import the required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Bo30Eu_-0ttv"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "from transformers import Seq2SeqTrainer\n",
        "from transformers import GPT2TokenizerFast\n",
        "from arabert.aragpt2.grover.modeling_gpt2 import GPT2LMHeadModel\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from evaluate import load\n",
        "from numpy import where\n",
        "from numpy import count_nonzero\n",
        "from numpy import mean"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1acFjhi6LL70"
      },
      "source": [
        "### 2. Initialize the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AC2WS1e-0w46"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at aubmindlab/aragpt2-base were not used when initializing GPT2LMHeadModel: ['ln_f.weight', 'ln_f.bias']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at aubmindlab/aragpt2-base and are newly initialized: ['emb_norm.weight', 'emb_norm.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_name = 'aubmindlab/aragpt2-base'\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name)\n",
        "\n",
        "rouge = load('rouge')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hpKhyFyeLPgb"
      },
      "source": [
        "### 3. Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MWo4c5I-_NRB"
      },
      "outputs": [],
      "source": [
        "def prepare_for_model(examples):\n",
        "  inputs = ['summarize: ' + doc for doc in examples['text']]\n",
        "  model_inputs = tokenizer(inputs, max_length=1024 * 10, truncation=True)\n",
        "\n",
        "  labels = tokenizer(text_target=examples['summary'], max_length=128 * 10, truncation=True)\n",
        "\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "  return model_inputs\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "  labels = where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "  prediction_lens = [count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "  result['gen_len'] = mean(prediction_lens)\n",
        "\n",
        "  return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii-DLE__LVjc"
      },
      "source": [
        "### 4. Specify the training arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JU8GuPJgkxn4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04f98312074c4d18a1ed8299438b8bc8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/212380 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5406a742f5224999ad518a3a2b5c669e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/53096 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_dataset = dataset.map(prepare_for_model, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ygQuotC1_Rpv"
      },
      "outputs": [],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/AIC-ICMTC/models/AraGPT2',\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy='epoch',\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=4,\n",
        "    save_strategy='epoch',\n",
        "    save_total_limit=2,\n",
        "    predict_with_generate=True\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NWdhB2bhLa8C"
      },
      "source": [
        "### 5. Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Z9yM7TKz_SKS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: summary, text. If summary, text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "c:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 212380\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 53096\n",
            "  Number of trainable parameters = 134994432\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80a80ddbdc844fda8439832a0234ed08",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/53096 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1498\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1500\u001b[0m )\n\u001b[1;32m-> 1501\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1502\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1503\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1504\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1505\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1506\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1747\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1748\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1749\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1751\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1752\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1753\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1754\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1755\u001b[0m ):\n\u001b[0;32m   1756\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2508\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2505\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2507\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2508\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   2510\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2511\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2540\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2538\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2539\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2540\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m   2541\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2542\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2543\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\arabert\\aragpt2\\grover\\modeling_gpt2.py:1274\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1265\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1266\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1267\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m return_dict \u001b[39m=\u001b[39m (\n\u001b[0;32m   1271\u001b[0m     return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m   1272\u001b[0m )\n\u001b[1;32m-> 1274\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m   1275\u001b[0m     input_ids,\n\u001b[0;32m   1276\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1277\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1278\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1279\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1280\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1281\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1282\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1283\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1284\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1285\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1286\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1287\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1288\u001b[0m )\n\u001b[0;32m   1289\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1291\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\arabert\\aragpt2\\grover\\modeling_gpt2.py:1100\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1090\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m   1091\u001b[0m         create_custom_forward(block),\n\u001b[0;32m   1092\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         encoder_attention_mask,\n\u001b[0;32m   1098\u001b[0m     )\n\u001b[0;32m   1099\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[0;32m   1101\u001b[0m         hidden_states,\n\u001b[0;32m   1102\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m   1103\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1104\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[0;32m   1105\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1106\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1107\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1108\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1109\u001b[0m     )\n\u001b[0;32m   1111\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1112\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\arabert\\aragpt2\\grover\\modeling_gpt2.py:582\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    580\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m    581\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> 582\u001b[0m feed_forward_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[0;32m    583\u001b[0m \u001b[39m# residual connection\u001b[39;00m\n\u001b[0;32m    584\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m feed_forward_hidden_states\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\arabert\\aragpt2\\grover\\modeling_gpt2.py:497\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    495\u001b[0m     \u001b[39mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[39m.\u001b[39mFloatTensor]]\n\u001b[0;32m    496\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor:\n\u001b[1;32m--> 497\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_fc(hidden_states)\n\u001b[0;32m    498\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(hidden_states)\n\u001b[0;32m    499\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(hidden_states)\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\transformers\\pytorch_utils.py:112\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    111\u001b[0m     size_out \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf,)\n\u001b[1;32m--> 112\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49maddmm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, x\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[0;32m    113\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(size_out)\n\u001b[0;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SPHxYHwDKMZe"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-JgeWLKnKJ6J"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5NopD95JKPSs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model 'GPT2LMHeadModel' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'T5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index -1 is out of bounds for dimension 1 with size 0",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m generation_pipeline \u001b[39m=\u001b[39m pipeline(\u001b[39m'\u001b[39m\u001b[39mtext2text-generation\u001b[39m\u001b[39m'\u001b[39m, model\u001b[39m=\u001b[39mmodel, tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[1;32m----> 3\u001b[0m result \u001b[39m=\u001b[39m generation_pipeline(\n\u001b[0;32m      4\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[0;32m      6\u001b[0m     num_beams\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m      7\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m     top_p\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m,\n\u001b[0;32m      9\u001b[0m     repetition_penalty \u001b[39m=\u001b[39;49m \u001b[39m3.0\u001b[39;49m,\n\u001b[0;32m     10\u001b[0m     no_repeat_ngram_size \u001b[39m=\u001b[39;49m \u001b[39m3\u001b[39;49m\n\u001b[0;32m     11\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:150\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    122\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    152\u001b[0m         \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m)\n\u001b[0;32m    153\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(el, \u001b[39mstr\u001b[39m) \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m args[\u001b[39m0\u001b[39m])\n\u001b[0;32m    154\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(res) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result)\n\u001b[0;32m    155\u001b[0m     ):\n\u001b[0;32m    156\u001b[0m         \u001b[39mreturn\u001b[39;00m [res[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result]\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:1074\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1072\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1073\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1074\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:1081\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1080\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1081\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1082\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1083\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:990\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[0;32m    989\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> 990\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m    991\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    992\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:172\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    170\u001b[0m generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m generate_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmax_length)\n\u001b[0;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_inputs(input_length, generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m], generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m--> 172\u001b[0m output_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    173\u001b[0m out_b \u001b[39m=\u001b[39m output_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Mohamed\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:1330\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[0;32m   1328\u001b[0m \u001b[39m# decoder-only models should use left-padding for generation\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n\u001b[1;32m-> 1330\u001b[0m     \u001b[39mif\u001b[39;00m pad_token_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39msum(inputs_tensor[:, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m==\u001b[39m pad_token_id) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1331\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m   1332\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1333\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration results, please set `padding_side=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` when initializing the tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1334\u001b[0m         )\n\u001b[0;32m   1336\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[0;32m   1337\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n",
            "\u001b[1;31mIndexError\u001b[0m: index -1 is out of bounds for dimension 1 with size 0"
          ]
        }
      ],
      "source": [
        "generation_pipeline = pipeline('text2text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "result = generation_pipeline(\n",
        "    '',\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    num_beams=10,\n",
        "    max_length=200,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty = 3.0,\n",
        "    no_repeat_ngram_size = 3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AusBlE2IKhLh"
      },
      "outputs": [],
      "source": [
        "result[0]['generated_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIawHqjBm49J"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
